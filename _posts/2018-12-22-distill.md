---
layout: distill
title: Learning Vector Quantization
description: Post en español
tags: distill formatting
giscus_comments: true
date: 2025-04-10
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: Fernando Silva


bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Teoría
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Código
  - name: Footnotes
  - name: Code Blocks
  - name: Interactive Plots
  - name: Mermaid
  - name: Diff2Html
  - name: Leaflet
  - name: Chartjs, Echarts and Vega-Lite
  - name: TikZ
  - name: Typograms
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Teoría

Learning Vector Quantization (LVQ) es un algoritmo de clasificación supervisada basado en prototipos, que busca representar cada clase mediante vectores representativos ('prototipos') que se ajustan iterativamente a los datos.

<p align="center">
  <img src="/assets/img/lvq_iman.gif" alt="LVQ Gif" width="500">
</p>
Los prototipos son como imanes: se acercan a los datos de su propia clase y se alejan de los de clases diferentes.

El procedimiento de aprendizaje del LVQ puede resumirse de la siguiente manera:

<p align="center">
  <img src="/assets/img/lvq_process.png" alt="LVQ Process" width="500">
</p>

Podemos pensar en los prototipos como algo similar a los centroides que se presentan en KMeans. Sin embargo, el ajuste iterativo es diferente. A diferencia de KMeans, que es un algoritmo no supervisado, LVQ es supervisado, ya que el ajuste de los prototipos depende no solo de la distancia, sino también de la clase de cada ejemplo.



Viendo el procesamiento a detalle tenemos lo siguiente:


<p align="center">
  <img src="/assets/img/lvq_process_detailed.pngg" alt="LVQ Detailed" width="500">
</p>



Añadiendo información a los procesos de LVQ tenemos:
* Inicialización de prototipos: Importante mencionar que el numero de prototipos es respecto al número de clases. La inicialización de protitpos puede realizarse con valores aleatorios, o tambien copiando un valor aleatorio de los registros presentes en el dataset.
* Ajuste iterativo basado en datos de entrenamiento: En este proceso se evidencia la clara diferencia con respecto a KMeans. Primero se debe calcular la distancia de un registro i con respecto a cada prototipo. Esta distancia puede ser euclidiana, manhattan, etc. Posteriormente nos enfocamos en el prototipo más cercano a dicho registro i. Si el prototipo más cercano es de la misma clase que el registro i se actualiza el valor del prototipo (w) con la siguiente formula:


$$
w(t+1) = w(t) + \alpha \cdot \bigl( x - w(t) \bigr)
$$

En otras palabras acercamos al prototipo al registro i, porque son la misma clase.
Por otro lado, si el prototipo mas cercano es de una clase diferente al registro i se actualiza el prototipo (w) para alejar al prototipo:


$$
w(t+1) = w(t) - \alpha \cdot \bigl( x - w(t) \bigr)
$$

* Clasificación por el prototipo: En la fase de clasificación, un nuevo registro es asignado a la clase de su prototipo más cercano, funcionando de manera similar a un clasificador k-NN, pero utilizando solo los prototipos como referencia.





---

## Código
La implementación de LVQ se puede  realizar de la siguiente manera:

1) Generamos un dataset sintético:
{% highlight python %}

import numpy as np

np.random.seed(42)
class1 = np.random.randn(50, 2) + [2, 2] 
class2 = np.random.randn(50, 2) + [6, 6]

data = np.vstack((class1, class2))
labels = np.array([0] * 50 + [1] * 50)

{% endhighlight %}

2) Definimos el modelo:

Inicializamos los parámetros necesarios para el funcionamiento de LVQ:

{% highlight python %}

n_classes = 2  
learning_rate = 0.1 
n_iterations = 100  

{% endhighlight %}

En este caso inicializamos los prototipos basandonos en una elección randómica en nuestro dataset:

{% highlight python %}

prototypes = np.array([data[labels == i][np.random.choice(len(data[labels == i]))] for i in range(n_classes)])

{% endhighlight %}

Definimos la distancia para el ajuste de los prototipos, en este caso la distancia euclideana:

{% highlight python %}

def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

{% endhighlight %}

Ahora definimos LVQ con numpy:

{% highlight python %}

for iteration in range(n_iterations):
    for i in range(len(data)):
        x = data[i] 
        y = labels[i]
        
        distances = np.array([euclidean_distance(x, prototype) for prototype in prototypes])
        closest_prototype_idx = np.argmin(distances)

        if closest_prototype_idx == y: 
            prototypes[closest_prototype_idx] = prototypes[closest_prototype_idx] + learning_rate * (x - prototypes[closest_prototype_idx])
        else: 
            prototypes[closest_prototype_idx] = prototypes[closest_prototype_idx] - learning_rate * (x - prototypes[closest_prototype_idx])

{% endhighlight %}

3) Definimos la función de clasificación:

{% highlight python %}
def classify(x, prototypes):
    distances = np.array([euclidean_distance(x, prototype) for prototype in prototypes])
    return np.argmin(distances)
{% endhighlight %}


4) [EXTRA] Para evaluar LVQ realizaremos lo siguiente:

{% highlight python %}
def classify(x, prototypes):
    distances = np.array([euclidean_distance(x, prototype) for prototype in prototypes])
    return np.argmin(distances)
{% endhighlight %}

5) [EXTRA] La visualización del rendimiento de LVQ se realiza a través de:

{% highlight python %}
import matplotlib.pyplot as plt

plt.scatter(class1[:, 0], class1[:, 1], color='blue', label='Clase 1')
plt.scatter(class2[:, 0], class2[:, 1], color='red', label='Clase 2')
plt.scatter(prototypes[:, 0], prototypes[:, 1], color='green', marker='x', s=100, label='Prototipos')
plt.legend()
plt.title("Datos y prototipos de LVQ")
plt.show()
{% endhighlight %}


Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

---

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

---

This line is separated from the one above by two newlines, so it will be a _separate paragraph_.

This line is also a separate paragraph, but...
This line is only separated by a single newline, so it's a separate line in the _same paragraph_.
