<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ferran23s.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ferran23s.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-13T23:56:28+00:00</updated><id>https://ferran23s.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Learning Vector Quantization</title><link href="https://ferran23s.github.io/blog/2025/lvq/" rel="alternate" type="text/html" title="Learning Vector Quantization"/><published>2025-04-10T00:00:00+00:00</published><updated>2025-04-10T00:00:00+00:00</updated><id>https://ferran23s.github.io/blog/2025/lvq</id><content type="html" xml:base="https://ferran23s.github.io/blog/2025/lvq/"><![CDATA[<h2 id="teoría">Teoría</h2> <p>Learning Vector Quantization (LVQ) es un algoritmo de clasificación supervisada basado en prototipos, que busca representar cada clase mediante vectores representativos (‘prototipos’) que se ajustan iterativamente a los datos.</p> <p align="center"> <img src="/assets/img/lvq_iman.gif" alt="LVQ Gif" width="500"/> </p> <p>Los prototipos son como imanes: se acercan a los datos de su propia clase y se alejan de los de clases diferentes.</p> <p>El procedimiento de aprendizaje del LVQ puede resumirse de la siguiente manera:</p> <p align="center"> <img src="/assets/img/lvq_process.png" alt="LVQ Process" width="500"/> </p> <p>Podemos pensar en los prototipos como algo similar a los centroides que se presentan en KMeans. Sin embargo, el ajuste iterativo es diferente. A diferencia de KMeans, que es un algoritmo no supervisado, LVQ es supervisado, ya que el ajuste de los prototipos depende no solo de la distancia, sino también de la clase de cada ejemplo.</p> <p>Viendo el procesamiento a detalle tenemos lo siguiente:</p> <p align="center"> <img src="/assets/img/lvq_process_detailed.png" alt="LVQ Detailed" width="900"/> </p> <p>Añadiendo información a los procesos de LVQ tenemos:</p> <ul> <li>Inicialización de prototipos: Importante mencionar que el numero de prototipos es respecto al número de clases. La inicialización de protitpos puede realizarse con valores aleatorios, o tambien copiando un valor aleatorio de los registros presentes en el dataset.</li> <li>Ajuste iterativo basado en datos de entrenamiento: En este proceso se evidencia la clara diferencia con respecto a KMeans. Primero se debe calcular la distancia de un registro i con respecto a cada prototipo. Esta distancia puede ser euclidiana, manhattan, etc. Posteriormente nos enfocamos en el prototipo más cercano a dicho registro i. Si el prototipo más cercano es de la misma clase que el registro i se actualiza el valor del prototipo (w) con la siguiente formula:</li> </ul> \[w(t+1) = w(t) + \alpha \cdot \bigl( x - w(t) \bigr)\] <p>En otras palabras acercamos al prototipo al registro i, porque son la misma clase. Por otro lado, si el prototipo mas cercano es de una clase diferente al registro i se actualiza el prototipo (w) para alejar al prototipo:</p> \[w(t+1) = w(t) - \alpha \cdot \bigl( x - w(t) \bigr)\] <ul> <li>Clasificación por el prototipo: En la fase de clasificación, un nuevo registro es asignado a la clase de su prototipo más cercano, funcionando de manera similar a un clasificador k-NN, pero utilizando solo los prototipos como referencia.</li> </ul> <hr/> <h2 id="código">Código</h2> <p>La implementación de LVQ se puede realizar de la siguiente manera:</p> <p>1) Generamos un dataset sintético:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">class1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> 
<span class="n">class2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">class1</span><span class="p">,</span> <span class="n">class2</span><span class="p">))</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">50</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span></code></pre></figure> <p>2) Definimos el modelo:</p> <p>Inicializamos los parámetros necesarios para el funcionamiento de LVQ:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">n_classes</span> <span class="o">=</span> <span class="mi">2</span>  
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span> 
<span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">100</span>  </code></pre></figure> <p>En este caso inicializamos los prototipos basandonos en una elección randómica en nuestro dataset:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">prototypes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">i</span><span class="p">][</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">i</span><span class="p">]))]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)])</span></code></pre></figure> <p>Definimos la distancia para el ajuste de los prototipos, en este caso la distancia euclideana:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span></code></pre></figure> <p>Ahora definimos LVQ con numpy:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> 
        <span class="n">y</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        
        <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prototype</span><span class="p">)</span> <span class="k">for</span> <span class="n">prototype</span> <span class="ow">in</span> <span class="n">prototypes</span><span class="p">])</span>
        <span class="n">closest_prototype_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">closest_prototype_idx</span> <span class="o">==</span> <span class="n">y</span><span class="p">:</span> 
            <span class="n">prototypes</span><span class="p">[</span><span class="n">closest_prototype_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">prototypes</span><span class="p">[</span><span class="n">closest_prototype_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">prototypes</span><span class="p">[</span><span class="n">closest_prototype_idx</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span> 
            <span class="n">prototypes</span><span class="p">[</span><span class="n">closest_prototype_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">prototypes</span><span class="p">[</span><span class="n">closest_prototype_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">prototypes</span><span class="p">[</span><span class="n">closest_prototype_idx</span><span class="p">])</span></code></pre></figure> <p>3) Definimos la función de clasificación:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prototypes</span><span class="p">):</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prototype</span><span class="p">)</span> <span class="k">for</span> <span class="n">prototype</span> <span class="ow">in</span> <span class="n">prototypes</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span></code></pre></figure> <p>4) [EXTRA] Para evaluar LVQ realizaremos lo siguiente:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prototypes</span><span class="p">):</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prototype</span><span class="p">)</span> <span class="k">for</span> <span class="n">prototype</span> <span class="ow">in</span> <span class="n">prototypes</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span></code></pre></figure> <p>5) [EXTRA] La visualización del rendimiento de LVQ se realiza a través de:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">class1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">class1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Clase 1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">class2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">class2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Clase 2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">prototypes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">prototypes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Prototipos</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Datos y prototipos de LVQ</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre></figure> <hr/> <h2 id="bibliografía">Bibliografía</h2> <ul> <li><a href="https://cse.engineering.nyu.edu/~mleung/CS6673/s09/LVQ.pdf">Leung, K. M. (2009). Learning Vector Quantization. Polytechnic University, Department of Computer and Information Science.</a></li> <li><a href="https://proceedings.neurips.cc/paper_files/paper/1995/file/9c3b1830513cc3b8fc4b76635d32e692-Paper.pdf">Sato, A., &amp; Yamada, K. (1995). Generalized Learning Vector Quantization. Advances in Neural Information Processing Systems, 8, 423–429</a></li> </ul>]]></content><author><name>Fernando Silva</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Post en español]]></summary></entry></feed>